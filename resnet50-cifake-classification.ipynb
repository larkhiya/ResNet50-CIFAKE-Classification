{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "486d36a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Enable CUDA GPU usage for NVIDIA GPUs\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Enable memory growth to avoid allocating all GPU memory at once\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        # Optionally restrict to specific GPU: tf.config.set_visible_devices(gpus[0], 'GPU')\n",
        "        print(f\"CUDA enabled: {len(gpus)} GPU(s) available\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU configuration error: {e}\")\n",
        "else:\n",
        "    print(\"No GPU found. Training will use CPU.\")\n",
        "\n",
        "# Check available devices\n",
        "print(\"\\nTensorFlow version:\", tf.__version__)\n",
        "print(\"Available devices:\")\n",
        "for device in tf.config.list_physical_devices():\n",
        "    print(f\"  - {device}\")\n",
        "\n",
        "# Enable mixed precision training for faster execution on GPU\n",
        "# Mixed precision (float16) provides significant speedup on NVIDIA GPUs with Tensor Cores\n",
        "try:\n",
        "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "    tf.keras.mixed_precision.set_global_policy(policy)\n",
        "    print(f'\\nMixed precision policy: {policy.name}')\n",
        "except Exception as e:\n",
        "    print(f'\\nMixed precision not available: {e}')\n",
        "\n",
        "# Enable XLA (Accelerated Linear Algebra) for faster execution on GPU\n",
        "try:\n",
        "    tf.config.optimizer.set_jit(True)\n",
        "    print('XLA compilation enabled')\n",
        "except Exception as e:\n",
        "    print(f'XLA not available: {e}')\n",
        "\n",
        "print('\\nGPU optimizations configured (CUDA + mixed precision + XLA)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b95be1b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-03T06:02:25.219190Z",
          "iopub.status.busy": "2025-11-03T06:02:25.218648Z",
          "iopub.status.idle": "2025-11-03T06:02:25.223154Z",
          "shell.execute_reply": "2025-11-03T06:02:25.222352Z"
        },
        "papermill": {
          "duration": 0.008501,
          "end_time": "2025-11-03T06:02:25.224246",
          "exception": false,
          "start_time": "2025-11-03T06:02:25.215745",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define dataset directory paths\n",
        "dataset_dir = \"./archive/\"\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "\n",
        "# Log resolved paths\n",
        "print(f\"Dataset train path: {train_dir}\")\n",
        "print(f\"Dataset test path: {test_dir}\")\n",
        "\n",
        "# Verify paths exist\n",
        "for name, path in [(\"Train\", train_dir), (\"Test\", test_dir)]:\n",
        "    assert os.path.exists(path), f\"{name} directory not found: {path}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c6c57d5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-03T06:02:25.230082Z",
          "iopub.status.busy": "2025-11-03T06:02:25.229626Z",
          "iopub.status.idle": "2025-11-03T06:02:27.051928Z",
          "shell.execute_reply": "2025-11-03T06:02:27.051141Z"
        },
        "papermill": {
          "duration": 1.826471,
          "end_time": "2025-11-03T06:02:27.053102",
          "exception": false,
          "start_time": "2025-11-03T06:02:25.226631",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Display sample images from each class\n",
        "classes = ['REAL', 'FAKE']\n",
        "\n",
        "def show_examples(base_dir, title):\n",
        "    \"\"\"\n",
        "    Display sample images from each class in the dataset.\n",
        "    \n",
        "    Args:\n",
        "        base_dir: Base directory containing class subdirectories\n",
        "        title: Title for the plot\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(10, 3))\n",
        "    fig.suptitle(title, fontsize=12)\n",
        "\n",
        "    for i, cls in enumerate(classes):\n",
        "        cls_dir = os.path.join(base_dir, cls)\n",
        "        if not os.path.exists(cls_dir):\n",
        "            print(f\"Warning: Class directory not found: {cls_dir}\")\n",
        "            continue\n",
        "            \n",
        "        # Get sample files from each class\n",
        "        all_files = os.listdir(cls_dir)\n",
        "        if len(all_files) < 2:\n",
        "            print(f\"Warning: Not enough files in {cls_dir}\")\n",
        "            continue\n",
        "            \n",
        "        sample_files = random.sample(all_files, 2)\n",
        "        for j, img_name in enumerate(sample_files):\n",
        "            img_path = os.path.join(cls_dir, img_name)\n",
        "            try:\n",
        "                img = mpimg.imread(img_path)\n",
        "                ax = axes[i*2 + j]\n",
        "                ax.imshow(img)\n",
        "                ax.set_title(f\"{cls}\")\n",
        "                ax.axis('off')\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display examples from train and test datasets\n",
        "show_examples(train_dir, \"Train Dataset Samples\")\n",
        "show_examples(test_dir, \"Test Dataset Samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb1ee02b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-03T06:02:27.061059Z",
          "iopub.status.busy": "2025-11-03T06:02:27.060574Z",
          "iopub.status.idle": "2025-11-03T06:08:23.211707Z",
          "shell.execute_reply": "2025-11-03T06:08:23.210637Z"
        },
        "papermill": {
          "duration": 356.159519,
          "end_time": "2025-11-03T06:08:23.216170",
          "exception": false,
          "start_time": "2025-11-03T06:02:27.056651",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Image preprocessing configuration\n",
        "IMG_HEIGHT = 32\n",
        "IMG_WIDTH = 32\n",
        "BATCH_SIZE = 64\n",
        "SEED = 42\n",
        "\n",
        "\n",
        "def load_training_dataset(\n",
        "    directory: str,\n",
        "    img_height: int = IMG_HEIGHT,\n",
        "    img_width: int = IMG_WIDTH,\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "    seed: int = SEED,\n",
        "):\n",
        "    \"\"\"Load training data from the given directory with preset image parameters.\"\"\"\n",
        "    return image_dataset_from_directory(\n",
        "        directory,\n",
        "        image_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=seed,\n",
        "        label_mode=\"binary\",  # binary classification\n",
        "    )\n",
        "\n",
        "\n",
        "# Create training dataset\n",
        "full_train_ds = load_training_dataset(train_dir)\n",
        "\n",
        "# Dataset split and pipeline configuration\n",
        "VAL_SPLIT = 0.1\n",
        "AUTOTUNE = tf.data.AUTOTUNE  # Automatically determines optimal parallelism\n",
        "\n",
        "\n",
        "def split_train_val(dataset, val_fraction: float = VAL_SPLIT):\n",
        "    \"\"\"Split a dataset into train/validation subsets by batches.\n",
        "\n",
        "    Returns train_ds, val_ds, total_batches, train_batches, val_batches.\n",
        "    \"\"\"\n",
        "    total_batches = tf.data.experimental.cardinality(dataset).numpy()\n",
        "    val_batches = int(total_batches * val_fraction)\n",
        "    train_batches = total_batches - val_batches\n",
        "\n",
        "    train_ds = dataset.take(train_batches)\n",
        "    val_ds = dataset.skip(train_batches)\n",
        "\n",
        "    return train_ds, val_ds, total_batches, train_batches, val_batches\n",
        "\n",
        "\n",
        "def load_test_dataset(\n",
        "    directory: str,\n",
        "    img_height: int = IMG_HEIGHT,\n",
        "    img_width: int = IMG_WIDTH,\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "):\n",
        "    \"\"\"Load test data from the given directory (no shuffling).\"\"\"\n",
        "    return image_dataset_from_directory(\n",
        "        directory,\n",
        "        image_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        label_mode=\"binary\",\n",
        "    )\n",
        "\n",
        "\n",
        "def optimize_dataset(dataset, shuffle: bool = False, seed: int | None = None, buffer_size: int = 1000):\n",
        "    \"\"\"Apply caching, optional shuffling, and prefetching to a dataset.\"\"\"\n",
        "    ds = dataset.cache()\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=buffer_size, seed=seed)\n",
        "    return ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "def count_images(dataset):\n",
        "    \"\"\"Count total number of images in a dataset.\"\"\"\n",
        "    return sum(batch[0].shape[0] for batch in dataset)\n",
        "\n",
        "\n",
        "# Split training data: 90% for training, 10% for validation\n",
        "train_ds, val_ds, total_batches, train_batches, val_batches = split_train_val(full_train_ds)\n",
        "\n",
        "# Load test data from test folder (not shuffled or modified)\n",
        "test_ds = load_test_dataset(test_dir)\n",
        "\n",
        "# Optimize datasets for maximum performance\n",
        "train_ds = optimize_dataset(train_ds, shuffle=True, seed=SEED)\n",
        "val_ds = optimize_dataset(val_ds)\n",
        "test_ds = optimize_dataset(test_ds)\n",
        "\n",
        "# Count images in each dataset split\n",
        "train_count = count_images(train_ds)\n",
        "val_count = count_images(val_ds)\n",
        "test_count = count_images(test_ds)\n",
        "\n",
        "# Print dataset summary\n",
        "print(\"Dataset Summary:\")\n",
        "print(f\"Total batches in train folder: {total_batches}\")\n",
        "print(f\"Train batches (90%): {train_batches}\")\n",
        "print(f\"Validation batches (10%): {val_batches}\")\n",
        "\n",
        "print(\"\\nNumber of images:\")\n",
        "print(f\"Train images: {train_count}\")\n",
        "print(f\"Validation images: {val_count}\")\n",
        "print(f\"Test images: {test_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0b925a5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-03T06:08:23.223898Z",
          "iopub.status.busy": "2025-11-03T06:08:23.223416Z",
          "iopub.status.idle": "2025-11-03T06:08:23.480997Z",
          "shell.execute_reply": "2025-11-03T06:08:23.480353Z"
        },
        "papermill": {
          "duration": 0.262883,
          "end_time": "2025-11-03T06:08:23.482418",
          "exception": false,
          "start_time": "2025-11-03T06:08:23.219535",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Data Augmentation (Applied only to training data)\n",
        "# Augmentation helps improve model generalization by creating variations of training images\n",
        "# Augmentation is applied on-the-fly (after caching) to ensure variety each epoch\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),  # Randomly flip images horizontally\n",
        "    layers.RandomRotation(0.1),       # Randomly rotate images up to 10%\n",
        "    layers.RandomZoom(0.1),           # Randomly zoom images up to 10%\n",
        "], name='data_augmentation')\n",
        "\n",
        "# Apply augmentation efficiently using map with parallel processing\n",
        "# Augmentation is applied on-the-fly (not cached) to ensure variety each epoch\n",
        "# This way we cache original images but get different augmentations each time\n",
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y),\n",
        "    num_parallel_calls=AUTOTUNE,  # Parallelize augmentation\n",
        "    deterministic=False  # Allow non-deterministic order for better performance\n",
        ")\n",
        "# Re-prefetch after augmentation to maintain pipeline efficiency\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34a44600",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-03T06:08:23.490943Z",
          "iopub.status.busy": "2025-11-03T06:08:23.490271Z",
          "iopub.status.idle": "2025-11-03T06:08:25.474375Z",
          "shell.execute_reply": "2025-11-03T06:08:25.473749Z"
        },
        "papermill": {
          "duration": 1.989389,
          "end_time": "2025-11-03T06:08:25.475627",
          "exception": false,
          "start_time": "2025-11-03T06:08:23.486238",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Visualize original vs augmented images to verify data augmentation is working correctly\n",
        "for images, labels in train_ds.take(1):\n",
        "    # Compute augmented batch once\n",
        "    augmented = data_augmentation(images, training=True)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for i in range(9):\n",
        "        # Original image\n",
        "        plt.subplot(3, 6, 2 * i + 1)\n",
        "        plt.imshow(tf.cast(images[i], tf.uint8))\n",
        "        plt.title(\"Original\", fontsize=8)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # Augmented image\n",
        "        plt.subplot(3, 6, 2 * i + 2)\n",
        "        plt.imshow(tf.cast(augmented[i], tf.uint8))\n",
        "        plt.title(\"Augmented\", fontsize=8)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.suptitle(\"Original (left) vs Augmented (right) Images\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    break  # Only show one batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c930f5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-03T06:08:25.484663Z",
          "iopub.status.busy": "2025-11-03T06:08:25.484459Z",
          "iopub.status.idle": "2025-11-03T06:08:25.487860Z",
          "shell.execute_reply": "2025-11-03T06:08:25.487287Z"
        },
        "papermill": {
          "duration": 0.008978,
          "end_time": "2025-11-03T06:08:25.488965",
          "exception": false,
          "start_time": "2025-11-03T06:08:25.479987",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Fix SSL certificate verification on macOS (required for Keras weight downloads)\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "base_model_loading",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ResNet50 base model with ImageNet pre-trained weights\n",
        "# Weights are downloaded automatically by Keras\n",
        "base_model = tf.keras.applications.ResNet50(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(img_height, img_width, 3),\n",
        "    pooling='avg'  # Changed to 'avg' for better performance\n",
        ")\n",
        "\n",
        "print(\"ResNet50 base model loaded with weights: imagenet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "488288ed",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-03T06:08:25.497553Z",
          "iopub.status.busy": "2025-11-03T06:08:25.497287Z",
          "iopub.status.idle": "2025-11-03T06:08:30.025489Z",
          "shell.execute_reply": "2025-11-03T06:08:30.024747Z"
        },
        "papermill": {
          "duration": 4.533777,
          "end_time": "2025-11-03T06:08:30.026630",
          "exception": false,
          "start_time": "2025-11-03T06:08:25.492853",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Build the complete model architecture\n",
        "# Make base model trainable for fine-tuning\n",
        "base_model.trainable = True\n",
        "\n",
        "# Define input layer\n",
        "inputs = tf.keras.Input(shape=(img_height, img_width, 3))\n",
        "\n",
        "# Pass input through base ResNet50 model\n",
        "x = base_model(inputs, training=False)\n",
        "\n",
        "# Add batch normalization for stable training\n",
        "x = layers.BatchNormalization()(x)\n",
        "\n",
        "# Add dense layers with regularization to prevent overfitting\n",
        "x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "x = layers.Dropout(0.4)(x)  # Dropout for regularization\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "\n",
        "# Output layer: single neuron with sigmoid activation for binary classification\n",
        "# Use float32 for output layer (required for mixed precision)\n",
        "outputs = layers.Dense(1, activation='sigmoid', dtype='float32')(x)\n",
        "\n",
        "# Create the complete model\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile model with optimizer, loss function, and metrics\n",
        "# Using Adam optimizer with learning rate scheduling\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',  # Binary cross-entropy for binary classification\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.Precision(),  # Precision metric\n",
        "        tf.keras.metrics.Recall()      # Recall metric\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Display model architecture summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "803998f0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-03T06:08:30.037085Z",
          "iopub.status.busy": "2025-11-03T06:08:30.036466Z",
          "iopub.status.idle": "2025-11-03T07:55:49.294955Z",
          "shell.execute_reply": "2025-11-03T07:55:49.294350Z"
        },
        "papermill": {
          "duration": 6439.26493,
          "end_time": "2025-11-03T07:55:49.296461",
          "exception": false,
          "start_time": "2025-11-03T06:08:30.031531",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Training with callbacks for better performance and convergence\n",
        "\n",
        "# EarlyStopping: Stop training if validation loss doesn't improve\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',           # Monitor validation loss\n",
        "    patience=20,                  # Wait 20 epochs before stopping\n",
        "    restore_best_weights=True,     # Restore weights from best epoch\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ReduceLROnPlateau: Reduce learning rate when validation loss plateaus\n",
        "# This helps fine-tune the model and improve convergence\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',           # Monitor validation loss\n",
        "    factor=0.5,                   # Reduce LR by 50%\n",
        "    patience=5,                   # Wait 5 epochs\n",
        "    min_lr=1e-7,                  # Minimum learning rate\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ModelCheckpoint: Save best model weights during training\n",
        "# This ensures we always have the best model even if training continues\n",
        "os.makedirs('./models', exist_ok=True)\n",
        "checkpoint_path = './models/best_model_weights.h5'\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_path,\n",
        "    monitor='val_loss',           # Monitor validation loss\n",
        "    save_best_only=True,          # Only save best model\n",
        "    save_weights_only=False,     # Save entire model\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model with all callbacks\n",
        "# Note: train_ds is already shuffled and cached\n",
        "history = model.fit(\n",
        "    train_ds,                     # Training dataset (already optimized)\n",
        "    validation_data=val_ds,       # Validation dataset\n",
        "    epochs=100,                   # Maximum number of epochs\n",
        "    callbacks=[early_stop, reduce_lr, model_checkpoint],  # All callbacks\n",
        "    verbose=1                     # Show training progress\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6b87718",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-03T07:55:57.316179Z",
          "iopub.status.busy": "2025-11-03T07:55:57.315902Z",
          "iopub.status.idle": "2025-11-03T07:55:57.625479Z",
          "shell.execute_reply": "2025-11-03T07:55:57.624709Z"
        },
        "papermill": {
          "duration": 4.338197,
          "end_time": "2025-11-03T07:55:57.627080",
          "exception": false,
          "start_time": "2025-11-03T07:55:53.288883",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Plot training history to visualize model performance\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot accuracy and loss curves from a Keras History object.\"\"\"\n",
        "    epochs = range(1, len(history.history[\"accuracy\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # Plot accuracy over epochs\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history.history[\"accuracy\"], label=\"Train\", marker=\"o\")\n",
        "    plt.plot(epochs, history.history[\"val_accuracy\"], label=\"Validation\", marker=\"s\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy over Epochs\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    # Plot loss over epochs\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history.history[\"loss\"], label=\"Train\", marker=\"o\")\n",
        "    plt.plot(epochs, history.history[\"val_loss\"], label=\"Validation\", marker=\"s\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss over Epochs\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(loc=\"upper right\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c1eb4d9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-03T07:56:05.699423Z",
          "iopub.status.busy": "2025-11-03T07:56:05.692475Z",
          "iopub.status.idle": "2025-11-03T07:56:32.961985Z",
          "shell.execute_reply": "2025-11-03T07:56:32.961201Z"
        },
        "papermill": {
          "duration": 31.419607,
          "end_time": "2025-11-03T07:56:32.963028",
          "exception": false,
          "start_time": "2025-11-03T07:56:01.543421",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Evaluate model performance on test data\n",
        "test_loss, test_acc, test_prec, test_rec = model.evaluate(test_ds, verbose=1)\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Precision: {test_prec:.4f}\")\n",
        "print(f\"Test Recall: {test_rec:.4f}\")\n",
        "\n",
        "# Get true labels from test dataset\n",
        "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
        "\n",
        "# Get predictions from model\n",
        "y_pred = model.predict(test_ds, verbose=1)\n",
        "# Convert probabilities to binary predictions using 0.5 threshold\n",
        "y_pred_classes = (y_pred.flatten() > 0.5).astype(\"int32\")\n",
        "\n",
        "# Create and display confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['FAKE', 'REAL'], \n",
        "            yticklabels=['FAKE', 'REAL'],\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes, target_names=['FAKE', 'REAL']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58d78d7b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-03T07:56:40.753284Z",
          "iopub.status.busy": "2025-11-03T07:56:40.753026Z",
          "iopub.status.idle": "2025-11-03T07:56:42.518209Z",
          "shell.execute_reply": "2025-11-03T07:56:42.517620Z"
        },
        "papermill": {
          "duration": 5.665925,
          "end_time": "2025-11-03T07:56:42.519595",
          "exception": false,
          "start_time": "2025-11-03T07:56:36.853670",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "os.makedirs('./models', exist_ok=True)\n",
        "model_save_path = \"./models/resnet50_binary.h5\"\n",
        "model.save(model_save_path)\n",
        "print(f\"Model saved successfully to: {model_save_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 3041726,
          "sourceId": 5256696,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 8636738,
          "sourceId": 13593196,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31154,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 6884.089013,
      "end_time": "2025-11-03T07:56:50.811396",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-11-03T06:02:06.722383",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
